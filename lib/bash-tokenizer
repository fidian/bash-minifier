#! /usr/bin/env bash

# shellcheck disable=SC1091
. bpm
bpm::include assign

bash-tokenizer::token() {
    local chunk noNewlines token

    token=$1
    chunk=$2
    BASH_TOKENIZER__TOKENS[${#BASH_TOKENIZER__TOKENS[@]}]="$token@$BASH_TOKENIZER__LINE:$BASH_TOKENIZER__CHAR $chunk"

    if [[ -n "${DEBUG-}" ]]; then
        printf "%-16s%-25s%4s:%-3s  %q\\n" "$token" "${BASH_TOKENIZER__CONTEXT[0]}" "$BASH_TOKENIZER__LINE" "$BASH_TOKENIZER__CHAR" "$chunk" >&2
    fi

    ((BASH_TOKENIZER__POS+=${#chunk}))
    noNewlines=${chunk//$'\n'}

    if [[ "${#noNewlines}" != "${#chunk}" ]]; then
        ((BASH_TOKENIZER__LINE+=${#chunk}-${#noNewlines}))
        chunk=${chunk##$'\n'}
        ((BASH_TOKENIZER__CHAR=1+${#chunk}))
    else
        ((BASH_TOKENIZER__CHAR+=${#chunk}))
    fi
}

bash-tokenizer::shebang() {
    local chunk

    if [[ ${BASH_TOKENIZER__CONTENT:0:2} == "#!" ]]; then
        chunk="${BASH_TOKENIZER__CONTENT/$'\n'*}"$'\n'
        bash-tokenizer::token SHEBANG "$chunk"
    fi
}

bash-tokenizer::comment() {
    local line

    # FIXME: Next line is inefficient
    line=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}
    line=${line/$'\n'*}
    bash-tokenizer::token COMMENT "$line"
}

bash-tokenizer::word() {
    local content

    content=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}

    # This is used to speed up the next replacement
    content=${content/$'\n'*}

    # Include some symbols as part of the word if not parsing a variable
    if ! bash-tokenizer::contextFlagExists VAR; then
        # shellcheck disable=SC2036
        content=${content/[$' \t${}[]()"|'"'"]*}
    else
        # shellcheck disable=SC2036
        content=${content/[$' \t${}[]()"|%'"'"]*}
    fi

    if [[ "$content" == *= ]]; then
        bash-tokenizer::token ASSIGN "$content"
    elif [[ -n "$content" ]]; then
        bash-tokenizer::token WORD "$content"
        bash-tokenizer::contextFlagAdd ARG
    else
        bash-tokenizer::token UNKNOWN "${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS:1}"
    fi
}

bash-tokenizer::ws() {
    local content

    content=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}

    # This is to speed up the next replacement
    content=${content/$'\n'*}
    content=${content/[^ $'\t']*}
    bash-tokenizer::token WS "$content"
}

bash-tokenizer::parenOpen() {
    local chunk line

    # FIXME: Next line is inefficient
    line=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}
    line=${line/$'\n'*}

    case "$line" in
        '('*)
            if [[ "$line" == '('*([[:space:]])')'* ]]; then
                chunk=${line/)*}
                bash-tokenizer::token EMPTY_LIST "$chunk)"
            else
                bash-tokenizer::token PAREN_OPEN '('
                bash-tokenizer::contextPush SUBSHELL
            fi
            ;;
    esac
}

bash-tokenizer::parenClose() {
    local line

    # FIXME: Next line is inefficient
    line=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}
    line=${line/$'\n'*}

    if [[ "$line" == "))"* ]] && [[ "${BASH_TOKENIZER__CONTEXT[0]}" == MATH.* ]]; then
        bash-tokenizer::token MATH_END '))'
        bash-tokenizer::contextPop
    elif [[ "${BASH_TOKENIZER__CONTEXT[0]}" == SUBSHELL.* ]]; then
        bash-tokenizer::token SUBSHELL_END ')'
        bash-tokenizer::contextPop
    else
        bash-tokenizer::token PAREN_CLOSE ')'
    fi
}

bash-tokenizer::braceOpen() {
    local line

    # FIXME: Next line is inefficient
    line=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}
    line=${line/$'\n'*}

    case "$line" in
        '{'*)
            bash-tokenizer::token BRACE_OPEN '{'
            bash-tokenizer::contextPush BLOCK
            ;;
    esac
}

bash-tokenizer::braceClose() {
    local line

    # FIXME: Next line is inefficient
    line=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}
    line=${line/$'\n'*}

    case "$line" in
        '}'*)
            bash-tokenizer::token BRACE_CLOSE '}'
            bash-tokenizer::contextPop
            ;;
    esac
}

bash-tokenizer::bracketOpen() {
    local line

    # FIXME: Next line is inefficient
    line=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}
    line=${line/$'\n'*}

    case "$line" in
        '[['*)
            bash-tokenizer::token D_BRACKET_OPEN '[['
            bash-tokenizer::contextPush D_BRACKET
            ;;

        '['*)
            bash-tokenizer::token BRACKET_OPEN '['
            bash-tokenizer::contextPush BRACKET
            ;;
    esac
}

bash-tokenizer::bracketClose() {
    local line

    # FIXME: Next line is inefficient
    line=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}
    line=${line/$'\n'*}

    case "$line" in
        ']]'*)
            bash-tokenizer::token D_BRACKET_CLOSE '[['
            bash-tokenizer::contextPop
            ;;

        ']'*)
            bash-tokenizer::token BRACKET_CLOSE ']'
            bash-tokenizer::contextPop
            ;;
    esac
}

bash-tokenizer::lessThan() {
    local line

    # FIXME: Next line is inefficient
    line=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}
    line=${line/$'\n'*}

    case "$line" in
        "<<"*)
            bash-tokenizer::token REDIR_HEREDOC '<<'
            bash-tokenizer::contextFlagAdd DOC
            ;;

        "<("*)
            bash-tokenizer::token REDIR_SUBSHELL '<('
            bash-tokenizer::contextPush SUBSHELL
            ;;

        '<'*)
            bash-tokenizer::token LT "<"
            ;;
    esac
}

bash-tokenizer::greaterThan() {
    # Need to work on this
    bash-tokenizer::token GT '>'
}

bash-tokenizer::heredocContent() {
    local content delim line tokenNum token

    tokenNum=${#BASH_TOKENIZER__TOKENS[@]}
    token=

    while [[ "$token" != REDIR_HEREDOC@* ]]; do
        ((tokenNum-=1))
        token=${BASH_TOKENIZER__TOKENS[tokenNum]}
    done

    # FIXME - This should consume all word-like tokens so delimiters like EOF''
    # and "a"b'c'$'d' all work.
    # FIXME - This doesn't handle <<- syntax.
    ((tokenNum+=1))
    token=${BASH_TOKENIZER__TOKENS[tokenNum]}
    delim=${token#* }
    delim=${delim//"'"}
    delim=${delim//'"'}
    content=
    line=

    while [[ "$line" != "$delim"$'\n' ]]; do
        content+=$line
        line=${BASH_TOKENIZER__CONTENT:$BASH_TOKENIZER__POS + ${#content}}
        line="${line/$'\n'*}"$'\n'
    done

    content+=$line
    bash-tokenizer::token HEREDOC "$content"
    bash-tokenizer::contextFlagRemove DOC
}

bash-tokenizer::ansiEscape() {
    local char pos

    pos=2

    while true; do
        # FIXME: Next line is inefficient
        char=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS + pos:1}

        # FIXME: Looping and reading a single character at a time is inefficient
        case "$char" in
            "'" | "")
                # FIXME: Next line is inefficient
                bash-tokenizer::token ANSI_ESCAPE "${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS:pos + 1}"

                return
                ;;

            "\\")
                ((pos+=2))
                ;;

            *)
                ((pos+=1))
                ;;
        esac
    done
}

bash-tokenizer::stringFragment() {
    local len

    len=0

    while true; do
        # FIXME: Next line is inefficient
        case "${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS + len:1}" in
            '"' | "\$" )
                if [[ "$len" != 0 ]]; then
                    # FIXME: Next line is inefficient
                    bash-tokenizer::token STR_FRAGMENT "${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS:len}"
                fi

                return
                ;;

            "\\")
                ((len+=2))
                ;;

            *)
                ((len+=1))
                ;;
        esac
    done
}

bash-tokenizer::stringStart() {
    bash-tokenizer::token STR_START '"'
    bash-tokenizer::contextFlagAdd STR
}

bash-tokenizer::stringEnd() {
    bash-tokenizer::token STR_END '"'
    bash-tokenizer::contextFlagRemove STR
}

bash-tokenizer::variable() {
    local line pos

    # FIXME: Next line is inefficient
    line=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS}
    line=${line/$'\n'*}

    case "$line" in
        "\$(("*)
            bash-tokenizer::token VAR_MATH "\$(("
            bash-tokenizer::contextPush MATH
            bash-tokenizer::contextFlagAdd VAR
            ;;

        "\$("*)
            bash-tokenizer::token VAR_SUBSHELL "\$("
            bash-tokenizer::contextPush SUBSHELL
            bash-tokenizer::contextFlagAdd VAR
            ;;

        "\${#"* | "\${!"*)
            bash-tokenizer::token VAR_BRACE "${line:0:3}"
            bash-tokenizer::contextPush BLOCK
            bash-tokenizer::contextFlagAdd VAR
            ;;

        "\${"*)
            bash-tokenizer::token VAR_BRACE "\${"
            bash-tokenizer::contextPush BLOCK
            bash-tokenizer::contextFlagAdd VAR
            ;;

        "\$'"*)
            bash-tokenizer::ansiEscape
            ;;

        "\$"*)
            pos=1

            # FIXME: Looping like this is inefficient
            while true; do
                case "${line:pos:1}" in
                    [[:word:]] | @ | "!")
                        ((pos+=1))
                        ;;

                    *)
                        bash-tokenizer::token VAR "${line:0:pos}"

                        return
                        ;;
                esac
            done

            ;;
    esac
}

bash-tokenizer::stringLiteral() {
    local content

    # FIXME: Next line is inefficient
    content=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS + 1}
    content=${content/"'"*}
    bash-tokenizer::token STR_LITERAL "'$content'"
}

bash-tokenizer::contextPop() {
    local oldIfs

    oldIfs=$IFS
    IFS=$'\n' # Needed for array slicing before Bash 4.0-rc1
    BASH_TOKENIZER__CONTEXT=("${BASH_TOKENIZER__CONTEXT[@]:1}")
    IFS=$oldIfs
}

bash-tokenizer::contextPush() {
    BASH_TOKENIZER__CONTEXT=("$1." ${BASH_TOKENIZER__CONTEXT[@]+"${BASH_TOKENIZER__CONTEXT[@]}"})
}

bash-tokenizer::contextFlagAdd() {
    BASH_TOKENIZER__CONTEXT[0]="${BASH_TOKENIZER__CONTEXT[0]//.$1./.}$1."
}

bash-tokenizer::contextFlagRemove() {
    BASH_TOKENIZER__CONTEXT[0]="${BASH_TOKENIZER__CONTEXT[0]//.$1./.}"
}

bash-tokenizer::contextFlagExists() {
    [[ "${BASH_TOKENIZER__CONTEXT[0]}" == *."$1".* ]]
}

bash-tokenizer::percent() {
    if bash-tokenizer::contextFlagExists VAR; then
        bash-tokenizer::token PERCENT %
    else
        bash-tokenizer::commandDefault
    fi
}

bash-tokenizer::commandDefault() {
    bash-tokenizer::word
}

bash-tokenizer::tokenizeString() {
    # FIXME: Next line is inefficient
    case "${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS:1}" in
        '"')
            bash-tokenizer::stringEnd
            ;;

        "\$")
            bash-tokenizer::variable
            ;;

        *)
            bash-tokenizer::stringFragment
            ;;
    esac
}

bash-tokenizer::tokenizeCommand() {
    local char

    # FIXME: Next line is inefficient
    char=${BASH_TOKENIZER__CONTENT:BASH_TOKENIZER__POS:1}

    case "$char" in
        "" | $'\n')
            bash-tokenizer::token EOL $'\n'
            bash-tokenizer::contextFlagRemove ARG

            if bash-tokenizer::contextFlagExists DOC; then
                bash-tokenizer::heredocContent
            fi
            ;;

        '(')
            bash-tokenizer::parenOpen
            ;;

        ')')
            bash-tokenizer::parenClose
            ;;

        '{')
            bash-tokenizer::braceOpen
            ;;

        '}')
            bash-tokenizer::braceClose
            ;;

        '[')
            bash-tokenizer::bracketOpen
            ;;

        ']')
            bash-tokenizer::bracketClose
            ;;

        '<')
            bash-tokenizer::lessThan
            ;;

        '>')
            bash-tokenizer::greaterThan
            ;;

        "#")
            bash-tokenizer::comment
            ;;

        " " | $'\t')
            bash-tokenizer::ws
            ;;

        "\$")
            bash-tokenizer::variable
            ;;

        "'")
            bash-tokenizer::stringLiteral
            ;;

        '"')
            bash-tokenizer::stringStart
            ;;

        '%')
            bash-tokenizer::percent
            ;;

        *)
            bash-tokenizer::commandDefault
            ;;
    esac
}

bash-tokenizer::tokenize() {
    local contentLength

    BASH_TOKENIZER__CONTENT=$2
    BASH_TOKENIZER__LINE=1
    BASH_TOKENIZER__CHAR=1
    BASH_TOKENIZER__POS=0
    BASH_TOKENIZER__CONTEXT=()
    BASH_TOKENIZER__TOKENS=()
    contentLength=${#2}

    bash-tokenizer::contextPush SOURCE
    bash-tokenizer::shebang

    while [[ "$BASH_TOKENIZER__POS" -lt "$contentLength" ]]; do
        if bash-tokenizer::contextFlagExists STR; then
            bash-tokenizer::tokenizeString
        else
            bash-tokenizer::tokenizeCommand
        fi
    done

    if [[ -n "${DEBUG-}" ]]; then
        if [[ "${#BASH_TOKENIZER__CONTEXT[@]}" != 1 ]]; then
            echo "WARNING: Terminating without cleaning context" >&2
            echo "${BASH_TOKENIZER__CONTEXT[@]}" >&2
        elif [[ "${BASH_TOKENIZER__CONTEXT[0]}" != SOURCE.* ]]; then
            echo "WARNING: Invalid terminating context" >&2
            echo "${BASH_TOKENIZER__CONTEXT[@]}" >&2
        fi
    fi

    local "$1" && assign::array "$1" ${BASH_TOKENIZER__TOKENS[@]+"${BASH_TOKENIZER__TOKENS[@]}"}
}
